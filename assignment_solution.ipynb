{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "ff700b74", "cell_type": "markdown", "source": "# Assignment Solutions (SVM, Na\u00efve Bayes, Spam Classification)\n\nThis notebook contains solutions to all questions (theory and practical) from the assignment.", "metadata": {}}, {"id": "07311581", "cell_type": "markdown", "source": "## Q1: What is a Support Vector Machine (SVM), and how does it work?\nSupport Vector Machine (SVM) is a supervised algorithm used for classification and regression. It finds the optimal hyperplane that maximizes the margin between classes. Support vectors are the data points closest to the hyperplane. Kernels allow non-linear classification.", "metadata": {}}, {"id": "20427fca", "cell_type": "markdown", "source": "## Q2: Difference between Hard Margin and Soft Margin SVM\n- **Hard Margin**: Assumes data is perfectly separable, no errors allowed, sensitive to outliers.\n- **Soft Margin**: Allows misclassification via slack variables, controlled by C. More robust to noise.", "metadata": {}}, {"id": "609f5f26", "cell_type": "markdown", "source": "## Q3: Kernel Trick in SVM\nThe kernel trick allows computing inner products in high-dimensional feature space without explicit mapping. Example: RBF kernel `K(x,x') = exp(-\u03b3||x-x'||^2)`. Use case: when decision boundaries are nonlinear.", "metadata": {}}, {"id": "59cf06bc", "cell_type": "markdown", "source": "## Q4: Na\u00efve Bayes Classifier and 'na\u00efve' assumption\nNa\u00efve Bayes uses Bayes\u2019 theorem with the assumption of conditional independence of features. Na\u00efve because it assumes features are independent given the class.", "metadata": {}}, {"id": "507a47d6", "cell_type": "markdown", "source": "## Q5: Types of Na\u00efve Bayes\n- **Gaussian NB**: For continuous features (assumes normal distribution).\n- **Multinomial NB**: For discrete counts (word counts, TF).\n- **Bernoulli NB**: For binary features (word present/absent).", "metadata": {}}, {"id": "1d11ab4c", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "## Q6: Iris + Linear SVM\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nsvm_lin = SVC(kernel='linear', probability=True)\nsvm_lin.fit(X_train, y_train)\ny_pred = svm_lin.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nsupport_vectors = svm_lin.support_vectors_\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Support vectors shape:\", support_vectors.shape)\nprint(\"Sample support vectors (up to 5):\", support_vectors[:5])\n", "outputs": []}, {"id": "e4348a5e", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "## Q7: Breast Cancer + GaussianNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\n\nbreast = datasets.load_breast_cancer()\nX, y = breast.data, breast.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\n\nprint(classification_report(y_test, y_pred, target_names=['malignant','benign'], zero_division=0))\n", "outputs": []}, {"id": "f5fc6cf0", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "## Q8: Wine + SVM GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nwine = datasets.load_wine()\nX, y = wine.data, wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nparam_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 0.01, 0.1]}\nsvc = SVC(kernel='rbf')\ngrid = GridSearchCV(svc, param_grid, cv=3, n_jobs=1)\ngrid.fit(X_train, y_train)\n\nbest_params = grid.best_params_\nbest_model = grid.best_estimator_\ny_pred = best_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Best hyperparameters:\", best_params)\nprint(\"Test Accuracy:\", accuracy)\n", "outputs": []}, {"id": "beeb7d48", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "## Q9: Text + MultinomialNB ROC-AUC\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score\n\ntry:\n    newsgroups = fetch_20newsgroups(subset='train',\n                                    categories=['alt.atheism','soc.religion.christian'],\n                                    remove=('headers','footers','quotes'))\n    texts, labels = newsgroups.data, newsgroups.target\n    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42, stratify=labels)\nexcept Exception:\n    import numpy as np\n    rng = np.random.RandomState(42)\n    vocab = ['god','church','science','atheist','belief','pray','team','win','game','policy','religion','text','email','offer','discount']\n    texts, labels = [], []\n    for i in range(300):\n        if i < 150:\n            w = rng.choice(vocab, size=rng.randint(5,12), replace=True)\n            w[:2] = ['atheist','science']\n            texts.append(\" \".join(w)); labels.append(0)\n        else:\n            w = rng.choice(vocab, size=rng.randint(5,12), replace=True)\n            w[:2] = ['church','pray']\n            texts.append(\" \".join(w)); labels.append(1)\n    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42, stratify=labels)\n\nvectorizer = TfidfVectorizer(stop_words='english', max_df=0.95)\nX_train_vec = vectorizer.fit_transform(X_train)\nX_test_vec = vectorizer.transform(X_test)\n\nmnb = MultinomialNB()\nmnb.fit(X_train_vec, y_train)\nproba = mnb.predict_proba(X_test_vec)[:,1]\nroc_auc = roc_auc_score(y_test, proba)\n\nprint(\"ROC-AUC:\", roc_auc)\n", "outputs": []}, {"id": "a9f4c8c6", "cell_type": "markdown", "source": "## Q10: Spam classification pipeline & business impact\n1. **Preprocessing**: clean text, handle missing, vectorize with TF-IDF, add features (URLs, caps ratio, etc.).  \n2. **Model choice**: start with MultinomialNB (fast), then LinearSVC or Logistic Regression.  \n3. **Imbalance handling**: class weights, resampling (SMOTE/undersample), threshold tuning.  \n4. **Evaluation**: precision, recall, F1, ROC-AUC, PR-AUC.  \n5. **Business impact**: reduces spam, avoids phishing, but false positives hurt trust \u2014 need monitoring & feedback loop.  \n\n**Sample pipeline:**\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.95)),\n    ('clf', LogisticRegression(class_weight='balanced', max_iter=500))\n])\n\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, stratify=labels, test_size=0.2)\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```\n", "metadata": {}}]}